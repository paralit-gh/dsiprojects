{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*r/pcgamingtechsupport* and *r/techsupport* are two well-frequented tech subreddits on reddit.com, where users often head to to seek advice for their tech-related problems. \n",
    "\n",
    "*r/techsupport* is a highly popular subreddit, intended for any general tech-related software or hardware issues. There is a plethora of problems that are routinely posted here, involving routers, Wi-Fi, monitors, motherboards, audio issues etc.\n",
    "\n",
    "*r/pcgamingtechsupport* is a more specialized subreddit, intended for PC users to post any game-related issues that they may have. These could include low in-game FPS, lag spikes, games not being able to boot, etc.\n",
    "\n",
    "As of 04 Dec 2020, *r/techsupport* boasts 1,248,988 subscribers while *r/pcgamingtechsupport* is far smaller, comprising 34,050 subscribers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim is to build a classification model that is able to classify posts into the two subreddits, and to uncover which are some of the top words/themes around each subreddit. \n",
    "\n",
    "Given that *r/pcgamingtechsupport* is essentially a more specialized version of *r/techsupport*, we would want to direct PC gaming users there instead, so they can obtain more personalized advice or faster responses from other PC gamers. At the same time, this allows for decluttering of *r/techsupport*, which would aid its moderators in better managing the rest of the posts.\n",
    "\n",
    "The classification model will be scored on its accuracy, i.e. the proportion of posts that it was able to classify correctly into the two subreddits.\n",
    "\n",
    "$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries and modules\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is used to scrape the posts from our selected subreddits, and outputs them into .csv files for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify urls of the apis we are going to use\n",
    "\n",
    "url1 = 'https://www.reddit.com/r/techsupport.json'\n",
    "url2 = 'https://www.reddit.com/r/pcgamingtechsupport.json'\n",
    "\n",
    "urls = [url1, url2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below code builds the pandas DataFrames from the jsons of the respective subreddits\n",
    "\n",
    "for index, url in enumerate(urls):\n",
    "    \n",
    "    posts = []\n",
    "    after = None\n",
    "\n",
    "    # run the loop 40 times to obtain 1000 posts    \n",
    "    for a in range(40):\n",
    "        if after == None:\n",
    "            current_url = url\n",
    "        else:\n",
    "            current_url = url + '?after=' + after\n",
    "        print(current_url)\n",
    "        res = requests.get(current_url, headers={'User-agent': 'Pony Inc 1.0'})\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            print('Status error', res.status_code)\n",
    "            break\n",
    "\n",
    "        # builds dictionary of posts sequentially           \n",
    "        current_dict = res.json()\n",
    "        current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "        posts.extend(current_posts)\n",
    "        after = current_dict['data']['after']\n",
    "\n",
    "        # concatenates the existing and new dataframes, and output to the same .csv each time the loop runs \n",
    "        # in case the code breaks halfway         \n",
    "        if a > 0:\n",
    "            prev_posts = pd.read_csv(str(index)+'.csv')\n",
    "            current_df = pd.DataFrame(current_posts)\n",
    "            new_df = pd.concat([prev_posts, current_df])\n",
    "            new_df.to_csv(str(index)+'.csv', index = False)\n",
    "        \n",
    "        # first time the loop is run, output to a .csv file \n",
    "        else:\n",
    "            pd.DataFrame(posts).to_csv(str(index)+'.csv', index = False)\n",
    "\n",
    "        # generate a random sleep duration to look more 'natural'\n",
    "        sleep_duration = random.randint(2,45)\n",
    "        print(sleep_duration)\n",
    "        time.sleep(sleep_duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
